# 0) 상사님이 말한 [ALP]를 “실제로 구현 가능한 문장”으로 바꾸면

상사님 요구를 실무 언어로 바꾸면 이렇게 됩니다.

1. **소리 유형 6~7개 선정**(예: 팬, 베어링, 밸브, 기어박스 등)
2. 고객에게 **데이터 수집 폼(녹음 방식 + 메타정보 + 최소 샘플 수)**을 제공
3. 고객이 **정상 소리(필수) + 가능하면 노이즈-only(권장)**를 준다
4. 우리 시스템(ALP)이 그 데이터를 받아서 **알고리즘 엔진(A/B/C) 중 최적을 선택/적용**
5. 결과로 **“이상 점수(알람)”를 내는 모델/규칙을 자동 생성**
6. 목표 KPI를 “95%”로 두되, 이 95%가 **AUC인지/오탐률인지/현장 운영 KPI인지**부터 명확히 정의해야 한다

지금 만든 4개 파일은, 이 중 **2~4번(데이터 계약→엔진 실행→결과 산출)**을 toy 데이터로 “끝까지” 흘려보내며 검증하는 뼈대입니다.

---

# 1) 먼저: 이 4개 파일이 하는 일(상사님 눈높이 버전)

## (1) `toy_dummy_data.py` — “고객이 준 데이터처럼 보이는” 시험용 데이터 만들기

* 실제 고객 데이터가 아직 없으니,
  **도메인 시프트(환경/노이즈/마이크/운전조건 변화)**가 들어간 “현장 같은” 데이터를 임의로 만들었습니다.
* 결과물:

  * `toy_data/` 안에 WAV 파일들
  * `toy_data/metadata.csv` (엑셀/CSV로 보는 ‘데이터 목록표’)

> 즉, **고객이 앞으로 우리에게 주게 될 데이터 패키지의 모양**을 미리 고정시킨 겁니다.

---

## (2) `approach_a.py` — 접근 A: “학습 없이도 바로 되는 초단기 온보딩 엔진”

* 고객이 정상 데이터 **몇 개(예: 3~10개)**만 줘도, “일단 돌게 만드는” 방식입니다.
* 핵심 아이디어는 간단히 말해:

  * 이미 학습돼 있는 AI(사전학습 AST)를 “귀”로 쓰고(학습은 안 함),
  * 정상 소리의 특징을 저장해두었다가,
  * 새 소리가 정상과 얼마나 다른지(거리/유사도)를 점수로 냅니다.
* 실제 사용한 사전학습 모델은 HuggingFace의 AST입니다. ([Hugging Face][1])
* 이번 toy 실행에서 A는 평균적으로 가장 나은 편이었습니다(단, toy이고 데이터가 작아 과해석 금지). `macro_auc_mean=0.665`가 기록돼 있습니다. 

---

## (3) `approach_b.py` — 접근 B: “현장/고객이 늘어날수록 유리한 ‘메모리뱅크’ 엔진”

* 고객/현장이 늘어나면 **환경이 달라서 오탐이 늘어나는 문제(도메인 시프트)**가 심해집니다.
* B는 “정상 데이터 기억장(메모리뱅크)”을 **소스(과거/다른현장)**와 **타깃(이번현장)**으로 나누어 만들고,

  * 새 소리가 정상 기억장과 얼마나 떨어져 있는지를 보고,
  * 도메인이 달라 점수 스케일이 바뀌는 문제를 정규화(Domain Normalization)로 완화합니다.
* toy 구현에서는 구조 검증을 위해 **가벼운 encoder(`logmel_stats`)**를 쓰고 있고, MemMixup(λ=0.9), DN 등을 켠 상태입니다. 
* 이번 toy에서는 A보다 낮고 C보다 높은 평균 성능이 나왔습니다(`macro_auc_mean≈0.55`). 

---

## (4) `approach_c.py` — 접근 C: “메타데이터가 잘 들어오는 고객에게 쓰는 고급 엔진”

* C는 “속도/부하/마이크위치/노이즈종류” 같은 **메타데이터를 이용해**,
  “조건 변화는 이상이 아니다”를 학습해두고, **3-shot 같은 극저샷에서도 빨리 적응**하려는 계열입니다.
* toy 구현은 Reptile(메타러닝) + 프로토타입 방식으로 구성돼 있고, `k_shot=3`을 명시합니다. 
* 이번 toy에선 평균 성능이 가장 낮게 나왔습니다(`macro_auc_mean=0.425`). 
  (이건 “C가 나쁘다”라기보다는, **C는 데이터/메타 품질과 학습 설계**가 맞아야 장점이 나오는 유형이라 toy에서 불리하게 보일 수 있습니다.)

---

# 2) 고객이 “어떤 데이터를 어떻게 준비해서” 우리에게 주는가 (폼 중심)

상사님이 원하신 “특정 form을 주고 받는다”를, 지금 파일들과 1:1로 연결해 설명하면 아래 형태가 가장 현실적입니다.

## 2.1 고객에게 주는 수집 폼(필수/권장/선택)

### A) 필수(최소 폼) — **이것만 있어도 Approach A로 MVP는 가능**

1. **소리 유형(machine_type)**: 6~7개 중 무엇인지

   * 예: `fan`, `bearing`, `valve` …
2. **설비/개체 ID(asset_id 또는 section_id)**

   * 같은 ‘팬’이라도 개체마다 정상 소리가 조금씩 달라서 구분이 필요합니다.
3. **정상 소리 WAV**

   * 권장 단위: **10초 클립**, 단일 채널(mono), 16 kHz(또는 원본이 48 kHz라도 제출 가능; 우리는 표준화 가능)
4. 각 WAV에 대해 최소한의 메타:

   * 녹음 시각(대략)
   * “정상(normal)”임을 보장하는지(작업자 확인)

### B) 권장(오탐 줄이는 핵심) — **노이즈-only / 설치조건**

5. **노이즈-only(기계 비가동, 환경 소리만)**

   * 같은 장소에서 30~120초만 있어도 “노이즈 때문에 이상으로 착각”하는 케이스를 줄이는 데 도움됩니다.
   * DCASE 2025 Task2 자체가 보조 데이터로 “클린 머신음 또는 노이즈-only”를 포함할 수 있게 설계합니다. ([DCASE][2])
6. **마이크 위치/방향(mic_position_id)**

   * 마이크 위치가 바뀌면 소리가 확 달라져서 오탐이 폭증하는 전형적인 원인입니다.
7. **운전 조건(속도/부하 구간)**

   * 가능하면 “저속/중속/고속” 정도의 구간 정보라도 좋습니다.

### C) 선택(고급 옵션) — **Approach C를 진지하게 쓰려면 사실상 필요**

8. 속도/부하/모드/설치/노이즈 타입 등 **메타데이터가 신뢰성 있게** 들어와야 합니다.

   * C는 “메타데이터 기반 적응”이 핵심이므로, 메타가 부정확하면 오히려 악화될 수 있습니다.

---

## 2.2 고객이 보내는 패키지(현실적 제안)

고객은 zip 파일 하나로 이렇게 주면 됩니다.

* `/audio/...` : WAV 파일들
* `/metadata.csv` : 각 WAV의 경로/ID/메타 정보를 담은 표 (엑셀로 만들어도 됨)

`toy_dummy_data.py`가 바로 이 형태(오디오 + metadata.csv)를 toy로 생성해줍니다.

---

# 3) 우리가 받으면 “무슨 일을 어떤 순서로” 하는가 (ALP 운영 흐름)

상사님이 원한 “자동 프로세스”를 지금 구현물 기준으로 단계화하면 아래입니다.

## Step 1) 데이터 수신 & 자동 점검(QA)

* 파일이 깨지지 않았는지, 길이가 너무 짧지 않은지, 무음/클리핑이 심하지 않은지 확인
* 메타데이터가 누락되었는지 확인
  → 이 단계는 실제 운영에서 오탐/재작업의 큰 원인을 줄입니다.

## Step 2) 표준화(전처리)

* 샘플레이트/길이 통일(가능하면 10초 단위로 잘라서 관리)
* 필요 시 노이즈-only 분리

## Step 3) 엔진 선택(시나리오에 따라 A/B/C)

* 아래 “4장”에서 설명하는 조건으로 A/B/C 중 선택

## Step 4) 결과 산출(고객에게 주는 결과물)

각 접근은 공통으로:

* `scores.csv`: “각 클립이 얼마나 이상한지” 점수표
* `metrics.json`: (라벨이 있을 때) 성능 요약 + 실험 설정 스냅샷
  을 만듭니다.

toy에서도 동일하게 나왔습니다:

* A 결과: AST 기반, `decision_quantile=0.05`, `support_n=3` 등 설정이 기록됨 
* B 결과: `encoder=logmel_stats`, `memmixup_lambda=0.9`, `enable_dn=true` 기록 
* C 결과: `train_mode=reptile`, `k_shot=3`, task 리스트 기록 

> 상사님 관점에서 핵심: “우리는 결국 점수와 리포트를 자동으로 뽑는다.”
> 그리고 “데이터가 쌓이면 더 고급 엔진으로 갈아탄다.”

---

# 4) A/B/C 적용 시나리오(상사님이 바로 의사결정할 수 있게)

아래는 “고객 상황 → 우리가 요구하는 데이터 → 어떤 엔진을 적용 → 기대/리스크”를 아주 단순화한 매핑입니다.

---

## 시나리오 A (가장 흔함): “신규 고객/신규 설비, 데이터 거의 없음”

**고객 상황**

* 새 설비라 데이터가 아직 없음
* 정상 데이터도 **10개 미만**(또는 1~2분)만 준비 가능
* 이상(고장) 데이터는 당연히 없음

**고객이 준비해서 주는 것**

* (필수) 정상 10초 클립 **3~10개**
* (권장) 노이즈-only **30~120초** (가능하면 같은 위치에서)

**우리가 하는 것**

1. `approach_a.py`로 바로 돌림
2. 정상 few-shot을 “기준”으로 저장
3. 새로 들어오는 소리가 정상과 다르면 점수↑
4. 알람 임계값은 “운영 정책”과 결합해 설정(예: 하루 알람 1건 이하)

**왜 A냐**

* 학습이 없어서(또는 거의 없어서) **과적합 위험이 낮고**, 온보딩이 매우 빠릅니다.

**리스크(냉정하게)**

* 노이즈/마이크/운전조건이 바뀌면 정상도 이상처럼 보일 수 있습니다.
  실제 toy에서도 특정 섹션(예: valve section 0 target)에서 target 도메인 AUC가 낮게 나오는 현상이 보입니다. 
  이런 케이스는 “소리 이상”이 아니라 “환경/조건 변화”가 점수를 흔드는 전형적 문제입니다.

---

## 시나리오 B: “고객이 늘고, 현장도 늘어서 ‘조건이 매번 달라’ 오탐이 문제”

**고객 상황**

* 기존 고객/기존 현장 데이터가 꽤 쌓였다(소스 데이터 有)
* 새 현장(타깃)은 정상 데이터가 몇 개(3~10개)뿐
* 환경이 달라져서 A만 쓰면 오탐이 늘 가능성이 있음

**고객이 준비해서 주는 것**

* (필수) 새 현장(타깃) 정상 3~10개
* (우리가 이미 가지고 있는 것) 기존 현장(소스) 정상 데이터(누적분)

**우리가 하는 것**

1. 소스 정상으로 “큰 정상 기억장” 구축
2. 타깃 정상은 적으니 MemMixup 같은 방식으로 보강(개념적으로)
3. 도메인별 점수 스케일 차이를 Domain Normalization으로 보정
   → toy B에서도 이 옵션들이 켜진 상태로 기록되어 있습니다. 

**왜 B냐**

* 현장이 늘수록 “정상이라는 개념”이 하나가 아니라 여러 개가 되기 때문에,
  **메모리뱅크 방식이 운영에 유리**해집니다.

**리스크/현실**

* 지금 toy B는 구조 검증을 위해 encoder를 가볍게 했습니다(`logmel_stats`). 
  즉, “논문급 성능”을 목표로 하면 B의 encoder는 사전학습 모델(BEATs/AST 등)로 강화하는 작업이 필요합니다.

---

## 시나리오 C: “메타데이터가 매우 잘 들어오는 대형 고객 + 장기 계약형”

**고객 상황**

* 속도/부하/모드/마이크 위치 등 메타데이터가 잘 관리됨
* 조건 변화가 잦아서 “조건 변화는 이상이 아니다”를 모델이 학습해야 함
* 운영 단계에서 신규 조건이 계속 생김(빠른 적응이 필요)

**고객이 준비해서 주는 것**

* (필수) 소스 정상 데이터가 어느 정도 필요(학습용)
* (필수) 타깃 정상 3~10개(온보딩/적응용)
* (권장) 메타데이터 품질(정확/누락 없음)이 핵심

**우리가 하는 것**

1. 소스 정상으로 메타러닝 학습(“적응하는 법”을 학습)
2. 타깃 few-shot으로 빠르게 적응
3. 프로토타입 기반 점수로 이상 판단
   toy에서도 `train_mode=reptile`, `k_shot=3`, task 리스트가 그대로 기록되어 있습니다. 

**왜 C냐**

* C는 “설치 위치/속도 변화 때문에 정상도 이상으로 오해”하는 문제를 정면으로 다루는 축입니다.
* 다만, A/B보다 운영 난이도가 높은 편이라 “메타데이터가 확실한 고객”에서 옵션으로 쓰는 게 현실적입니다.

---

# 5) 상사님이 물어볼 “DB(데이터) 얼마나 필요하냐?”를 기준/근거로 답하면

여기서 DB는 보통 2가지 의미가 있습니다.

1. **얼마나 많은 음원(분/시간/클립)이 필요하냐**
2. **저장 용량이 얼마나 필요하냐(스토리지/DB 비용)**

둘 다 근거를 붙여 설명드리겠습니다.

---

## 5.1 “클립 수/시간” 기준(벤치마크에서 확인되는 스케일)

### (1) DCASE 2021 Task2 (도메인 시프트 + few-shot 타깃)

* 섹션당 **소스 정상 약 1,000개**, **타깃 정상 3개**가 제공되는 구성이 공식 설명으로 존재합니다. ([Zenodo][3])
  → 즉, 연구 벤치마크 자체가 “타깃 정상 3개로 적응”을 공식 전제로 합니다.

### (2) DCASE 2025 Task2 (first-shot + 보조데이터)

* 섹션당 **소스 정상 990 + 타깃 정상 10**, 그리고 **보조 100개(클린 머신음 또는 노이즈-only)**를 명시합니다. ([DCASE][2])
  → “노이즈-only를 별도 데이터로 받는 게 최신 벤치마크 설계에 포함”되어 있습니다.

### (3) DCASE 2020 Task2 (normal-only ASD 기본 스케일)

* 머신 ID당 정상 학습 약 1,000개, 테스트 정상/이상 100~200개 정도라는 기술 보고서 언급이 있습니다. ([DCASE][4])

### (4) MIMII 공개 데이터셋(현실 기계음 공개셋)

* 모델(제품 모델)별 정상 소리 **5,000~10,000초**, 이상 소리 약 **1,000초**가 포함된다고 Zenodo에 명시돼 있습니다. ([Zenodo][5])
  → 정상만 해도 **약 1.4~2.8시간** 수준입니다.

**요약(과장 없이)**

* “정상 3~10개로도 돌아가는” 방식(A/B)은 벤치마크에도 존재합니다. ([Zenodo][3])
* 하지만 “성능을 안정적으로 높이려면” 결국 정상 데이터는 **분 단위가 아니라 시간 단위로 늘어나는 경향**이 공개 데이터에도 나타납니다. ([Zenodo][5])

---

## 5.2 “저장 용량” 감(현실적 산정)

가장 많이 쓰는 표준을 가정하면:

* 16 kHz, mono, 16-bit PCM, 10초 WAV
  → 대략 **0.32 MB/클립**(대략치)

이를 이용해 예시로 계산하면:

### (A) 콜드스타트(시나리오 A) — 매우 작음

* 정상 10초 클립 10개: 약 3.2MB
* 노이즈-only 120초(10초×12개): 약 3.8MB
  → **자산(설비) 1대당 10MB 내외**로도 MVP는 가능

### (B) 운영 안정화(권장) — 여전히 크지 않음

* “운전 조건 3개(저/중/고)” × 각 조건당 1~3분

  * 1분 = 10초×6개 = 6클립 → 1.9MB
  * 3분 = 18클립 → 5.8MB
* 예: 3조건 × 3분 = 54클립 → 약 17MB + 노이즈 데이터
  → **설비 1대당 수십 MB**로 운영 안정성 개선 가능

### (C) 벤치마크급(학습/검증까지 강하게) — 커짐

* DCASE 2025 섹션 1개 기준 학습 정상만 해도 약 1,000클립 내외로 설계됩니다(990+10). ([DCASE][2])
* 1,000클립 × 0.32MB ≈ 320MB
  → 섹션(설비) 단위가 많아지면 빠르게 GB 단위로 증가

---

# 6) “95% 자동 모델 개발”을 냉정하게 말하면, 지금 단계에서 가능한 것/불가능한 것

## 지금 가능한 것(이미 코드로 증명된 범위)

* 고객 데이터가 위 폼대로 오면,

  * **A/B/C 중 최소 1개 엔진으로 점수 산출은 자동화 가능**
  * 결과는 `scores.csv`로 정리됨(각 클립별 이상 점수)
* toy에서도 실제로 A/B/C 실행 결과가 `metrics.json`으로 남았습니다:

  * A: `macro_auc_mean=0.665` 
  * B: `macro_auc_mean≈0.55` 
  * C: `macro_auc_mean=0.425` 

## 아직 “95% 자동”이라고 말하면 안 되는 이유(현실)

1. **95%의 정의부터 합의 필요**

   * 연구 지표(AUC/pAUC) 95%인지,
   * 운영 KPI(오탐 하루 1건 이하, 미탐률 등)인지가 다릅니다.
2. 정상-only로 시작하면, **“실제로 95%인지 검증할 라벨(이상 사례)이 부족**합니다.

   * 즉, “돌아간다”와 “95%다”는 다른 문제입니다.
3. 도메인 시프트/노이즈가 강하면, toy에서도 보이듯 정상도 이상처럼 점수가 튈 수 있습니다.

   * 그래서 **노이즈-only/메타데이터/운전조건 커버**가 중요합니다(벤치마크도 그 방향). ([DCASE][2])

---

# 7) 상사님께 제안하는 “ALP 제품화” 운영 전략(현실적)

1. **기본값은 A(즉시 온보딩)**

   * 고객에게 요구하는 데이터가 가장 작고, 즉시 돌아감
2. 고객 데이터가 누적되면 **B를 자동 옵션으로**

   * 현장/고객이 늘어날수록 메모리뱅크 방식이 유리
3. 메타데이터가 잘 들어오는 대형 고객에 한해 **C는 고급 옵션**

   * “조건 변화 = 이상” 오탐을 줄이는 방향이지만, 운영 난이도/학습 비용이 큼
4. 공통 산출물은 고정

   * `scores.csv`(점수표) + 운영 임계값 + 리포트(추후 자동화)
5. 데이터 계약(폼)만 엄격히

   * 여기만 엄격하면 A/B/C 엔진은 뒤에서 갈아끼우는 구조가 됩니다.

---

원하시면 다음 단계로, **상사님/고객에게 바로 줄 수 있는 1페이지짜리 “데이터 수집 폼(체크리스트)”**를 만들어 드릴 수 있습니다.
(예: “팬 소리의 경우: 정상 10초×10개, 노이즈-only 2분, 마이크 위치 1~2개, 속도 저/중/고 각각 1분” 같은 형태로, 6~7개 소리 유형별로 템플릿화)

[1]: https://huggingface.co/MIT/ast-finetuned-audioset-10-10-0.4593?utm_source=chatgpt.com "MIT/ast-finetuned-audioset-10-10-0.4593"
[2]: https://dcase.community/challenge2025/task-first-shot-unsupervised-anomalous-sound-detection-for-machine-condition-monitoring?utm_source=chatgpt.com "First-Shot Unsupervised Anomalous Sound Detection for ..."
[3]: https://zenodo.org/records/4562016?utm_source=chatgpt.com "DCASE 2021 Challenge Task 2 Development Dataset"
[4]: https://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Phan_94_t2.pdf?utm_source=chatgpt.com "dcase 2020 task 2: unsupervised detection of anomalous ..."
[5]: https://zenodo.org/records/3384388?utm_source=chatgpt.com "MIMII Dataset: Sound Dataset for Malfunctioning Industrial ..."
