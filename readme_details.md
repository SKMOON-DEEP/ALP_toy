

# (1) `toy_dummy_data.py` — 어떤 가상의 dummy 샘플을 만들었나

## 1-1. 이 스크립트가 만드는 데이터셋의 목적(핵심 이론)

현장에서 ASD(이상음 탐지)가 어려운 이유는 보통 **“진짜 이상”보다 “환경/조건 변화(도메인 시프트)”가 더 크게 소리를 바꾼다**는 점입니다.
DCASE Task 2류 벤치마크도 이 문제를 정면으로 다루며, 최근 DCASE 2025 Task 2는 **노이즈-only(공장 소음만) 또는 클린 머신음 같은 추가 데이터 활용을 옵션으로 공식 허용**합니다.

`toy_dummy_data.py`는 이 현실을 “축소판”으로 재현하려고:

* **기계 종류(machine_type)** 여러 개를 동시에 커버하고
* 기계가 같은 종류라도 개체/라인 차이를 흉내내는 **section_id**를 만들고
* **source 도메인(기존 조건)** vs **target 도메인(새 조건)** 을 나눠서
* target은 정상 데이터가 **few-shot(아주 적게)**만 있다고 가정하고
* 테스트에는 **정상/이상**이 섞여 있게 생성합니다.

---

## 1-2. 이번 실행(scale=0.05)에서 확정된 “데이터 구조” (숫자/구성)

`approach_b`의 `metrics.json`에, 섹션마다 다음이 명시돼 있습니다:

* **source train normal = 50**
* **target train normal = 3**
* **test = 20** 

또한 각 섹션의 도메인별 테스트는:

* source test: 10개(그 중 anomaly 5개)
* target test: 10개(그 중 anomaly 5개)

이건 A/B/C 세 결과 json에서도 동일 패턴으로 보입니다.

즉, **“타깃 정상 3-shot”**이라는 강한 조건으로 돌아가게 되어 있고(현장 cold-start에 가까움), 이 조건은 DCASE 2021 Task2가 Zenodo 데이터 설명에서 “타깃 정상 3개”를 명시한 구성과 정합합니다.

---

## 1-3. 생성된 “메타데이터(조건)”는 무엇인가

`scores.csv`의 `relative_path`를 보면, 파일명에 이런 속성이 들어갑니다(예시 한 줄):

* `...section_00_source_test_normal_0001_sp1_ld1_mic0_noi3_snr26p3.wav`

여기서:

* `section_00` : 같은 기계 타입 내 개체/라인(평가 단위)
* `source / target` : 도메인(조건군)
* `train / test` : 분할
* `normal / anomaly` : 라벨
* `spX` : speed_level (속도 구간)
* `ldX` : load_level (부하 구간)
* `micX` : mic_position_id (마이크 위치/설치 차이)
* `noiX` : noise_id (노이즈 종류)
* `snrYYpZ` : SNR(소음 세기)

이 구성 자체가 **Approach C(메타데이터 기반 적응)**가 “학습할 수 있는 정보”를 제공하고, A/B에서도 “도메인 시프트 원인을 분석”할 수 있게 해줍니다.

---

## 1-4. toy에서 “도메인 시프트”는 어떻게 들어가 있나 (핵심)

`out_a_scores.csv`의 파일명(=속성)을 파싱해보면, test에서 도메인별 분포가 “의도적으로 다르게” 잡혀 있습니다(요약):

* **source 도메인**

  * speed: 낮은 구간 중심(sp0~sp2)
  * load: ld0~ld1
  * mic: mic0~mic1
  * noise: noi0/1/3
  * 평균 SNR이 더 높음(=더 깨끗함)

* **target 도메인**

  * speed: 높은 구간 중심(sp2~sp4)
  * load: ld1~ld2
  * mic: mic2~mic3
  * noise: noi2/4/5
  * 평균 SNR이 더 낮음(=더 시끄러움)

이게 의미하는 바는 간단합니다.

> **target 정상 소리**는, 소리 자체가 고장나지 않았더라도(정상이라도)
> “마이크/속도/노이즈가 바뀌어서” source 정상과 꽤 달라 보이게 설계되어 있다는 뜻입니다.

이 상황은 실제 현장에서 정말 흔하고(마이크 위치가 바뀌면 파형/스펙트럼이 크게 변함), 도메인 시프트가 “미세한 이상보다 더 큰 변화”가 될 수 있다는 문제의식은 DCASE 도메인 시프트 리뷰에서도 강조됩니다.

---

## 1-5. “anomaly(이상)”는 어떤 성격으로 만들어졌나

파일명/라벨 구조상, toy는 **정상(normal)**과 **이상(anomaly)**을 별도로 생성합니다(테스트에 도메인별로 5개씩 존재).
다만 “이상이 어떤 수학적 변형(톤 추가/임펄스/AM 변화 등)인지”는, 여기 대화에 코드 원문이 없으므로 100% 단정할 수는 없습니다.

현실적으로는 보통 toy ASD 생성기는 다음 중 1~2개를 섞어 “이상”을 흉내냅니다(직관용 설명):

* **추가 톤/휘파람(좁은 대역 성분)**
* **주기성 붕괴/진동 불안정(회전체 느낌 변화)**
* **딱딱 끊기는 임펄스(베어링 결함/밸브 충격 같은 느낌)**
* **대역 전체 에너지 증가(마찰/진동 증가 느낌)**

toy의 진짜 가치는 “이상 패턴이 실물과 얼마나 닮았냐”보다, **도메인 시프트와 few-shot 조건이 모델을 흔드는지**를 빠르게 확인하는 데 있습니다.

---

# (2) `approach_a.py` — 핵심 알고리즘 (Frozen AST + Patch Similarity)

## 2-1. 한 문장으로

**정상 few-shot(예: 3개)에서 나온 “패치 특징 조각들”을 메모리로 저장해두고, 새 소리의 패치가 그 조각들과 얼마나 안 닮았는지(코사인 유사도)로 이상 점수를 만든다.**

이 방법은 **학습이 거의 없거나(또는 아예 없고)**, few-shot에서 과적합 위험이 낮다는 장점이 있습니다.

---

## 2-2. 특징(Feature) 추출 — AST가 하는 일

AST(Audio Spectrogram Transformer)는 **스펙트로그램을 이미지처럼 보고** ViT 방식으로 **16×16 패치**로 쪼갠 뒤, Transformer로 패치별 임베딩을 뽑는 구조입니다.
그리고 본 구현은 HuggingFace의 AST 문서에도 있는 대표 체크포인트(“MIT/ast-finetuned-audioset-10-10-0.4593”)를 사용합니다.

---

## 2-3. “정상 특징 저장 방식” (Key Memory)

Approach A는 섹션마다 **support_domain=target**, **support_n=3**(타깃 정상 3개)를 사용합니다. 

Step-by-step:

1. support 정상 3개를 AST에 넣어 **레이어별 패치 임베딩**을 얻습니다.
2. 각 레이어에서 나온 모든 패치 임베딩을 모아서
   **Key memory (K_l)** (레이어 (l)의 정상 패치 사전)을 만듭니다.
3. 벡터는 **L2 normalize**해서 코사인 유사도 계산이 안정되게 합니다. 
4. 필요하면 메모리 크기를 줄이기 위해 subsampling도 가능하게 설계돼 있고, 현재 설정은 `key_subsample="random"`입니다. 

> 직관: 정상 소리를 “사진 앨범”으로 만든다고 생각하면,
> AST는 소리를 작은 패치 사진 조각들로 쪼개고, 그 조각들의 특징을 앨범에 저장하는 겁니다.

---

## 2-4. “거리/유사도 방식” (Cosine similarity + max-match)

테스트 클립 (x)가 들어오면:

1. AST로 패치 임베딩 (q_{p,l})들을 얻음

   * (p)는 패치 인덱스, (l)은 레이어 인덱스
2. 각 패치 (q_{p,l})에 대해, 정상 key memory (K_l)와의 코사인 유사도 중 **최대값**을 구합니다:

[
\text{sim}*{p,l} = \max*{k \in K_l} \cos(q_{p,l}, k)
]

3. 패치 이상도를 다음처럼 정의합니다:

[
a_{p,l} = 1 - \text{sim}_{p,l}
]

즉, **정상 패치들과 “가장 닮은 것”조차 별로 안 닮았으면**(max cos가 낮으면), 그 패치는 이상으로 봅니다.

4. 레이어는 `use_layers="all"`, `exclude_last_layer=true`로 되어 있어, 여러 레이어 정보를 평균/집계합니다. 

---

## 2-5. 패치→클립 점수로 모으는 방식(Quantile pooling)

패치 이상도 (a_{p})들이 클립마다 수백~수천 개 나오는데, 여기서 **상위 일부만** 집계합니다.

* 설정: `decision_quantile=0.05`, `quantile_tail="upper"` 

직관적으로는:

* “전체 패치 중 **가장 이상한 5%**가 정말 이상이라면, 그 클립도 이상일 가능성이 높다”
* 정상 소리에도 순간 잡음/짧은 흔들림이 있을 수 있으니, **전체 평균**보다 “상위 꼬리”가 더 민감하게 이상을 잡습니다.

---

## 2-6. 직관 예시

* 정상 팬 소리: 일정한 바람 소리 + 회전 톤
* 고장 팬 소리: 특정 순간에 “삐-” 같은 날카로운 톤이 섞임

이때 “삐-”가 나타난 구간의 패치 임베딩은 정상 앨범의 어떤 패치와도 잘 안 맞아서(최대 코사인 유사도가 낮아져), 그 패치 이상도가 커지고, 상위 5% pooling에 의해 클립 점수가 올라갑니다.

---

# (3) `approach_b.py` — 핵심 알고리즘 (Memory Bank + kNN + MemMixup + Domain Normalization)

## 3-1. 한 문장으로

**정상 소리 임베딩을 “기억장(메모리뱅크)”으로 쌓아두고, 새 소리가 그 기억장으로부터 얼마나 떨어졌는지(kNN 거리)로 이상을 판단하되, source/target 점수 스케일 차이를 Z-score로 맞추고, target이 너무 적으면 MemMixup으로 target 기억장을 보강한다.**

이 구성은 GenRep 논문이 제시한 핵심 흐름과 일치합니다.

---

## 3-2. 메모리뱅크(Memory Bank)를 어떻게 구현하나

섹션마다:

* source 정상 임베딩 집합 (M_s) (이번 toy에선 50개)
* target 정상 임베딩 집합 (M_t) (이번 toy에선 3개)

을 만듭니다. 

toy 구현에서 임베딩은 `encoder="logmel_stats"`, 차원은 `encoder_dim=256`으로 기록돼 있습니다. 
(이건 “파이프라인 검증용 가벼운 인코더”이고, GenRep 원 논문은 BEATs 같은 강한 사전학습 특징을 사용합니다. )

---

## 3-3. 거리(kNN distance)는 어떻게 구하나

테스트 임베딩 (z)에 대해:

* source 거리:

[
d_s(z) = \text{kNN-dist}(z, M_s)
]

* target 거리:

[
d_t(z) = \text{kNN-dist}(z, M_t)
]

GenRep는 “소스/타깃 메모리뱅크에 대해 각각 kNN 거리를 계산”한다고 명시합니다.

현재 구현 설정은 `knn_k=1`이라, 사실상 “가장 가까운 정상 샘플까지의 거리”입니다. 
그리고 `scores.csv`에도 이 값이 `ds`, `dt`로 그대로 저장됩니다(디버깅에 매우 유용).

> 직관: “정상 지도(map)”를 만들어두고, 새 소리가 그 지도에서 얼마나 멀리 떨어져 있는지 보는 겁니다.

---

## 3-4. MemMixup은 target을 어떻게 보강하나 (핵심 포인트)

타깃 정상 샘플이 너무 적으면((|M_t| \ll |M_s|)), kNN 기반 점수가 불안정해질 수 있습니다.
GenRep는 이를 보완하기 위해 **MemMixup**을 제안하며:

* 각 target 특징 (t)에 대해, source에서 가장 가까운 (K)개를 찾고,
* 다음처럼 보간한 특징을 target bank에 추가합니다:

[
\tilde{t} = \lambda t + (1-\lambda) s
]

그리고 논문에서 (\lambda)를 **명시적으로 0.9로 설정**합니다.

현재 구현도 동일하게:

* `enable_memmixup=true`
* `memmixup_k=5`
* `memmixup_lambda=0.9` 
  로 기록돼 있습니다.

> 직관: target 정상 3개는 “너무 점이 적은 지도”라 불안정하니,
> source에서 가까운 점을 살짝 섞어서 target 주변에 “가짜 정상 점”을 더 찍어주는 것입니다.
> 단, (\lambda=0.9)라서 **target 쪽을 더 믿고** 섞습니다.

---

## 3-5. Domain Normalization(Z-score)은 어떻게 하나 (핵심 포인트)

도메인이 바뀌면 거리의 스케일 자체가 달라질 수 있습니다(예: target이 더 시끄러워서 전체적으로 거리가 커지는 등).
GenRep는 이를 막기 위해:

* source score와 target score를 **각각 Z-score로 정규화**하고
* 최종 점수는 **정규화된 두 값 중 더 작은 것(min)**을 쓰는 방식을 제안합니다.

즉,

[
z_s = \frac{d_s - \mu_s}{\sigma_s + \epsilon},\quad
z_t = \frac{d_t - \mu_t}{\sigma_t + \epsilon}
]
[
\text{score}(x) = \min(z_s, z_t)
]

현재 구현도:

* `enable_dn=true`
* `dn_eps_std=1e-06`
* `dn_use_loocv=true` 
  로 DN을 켠 상태입니다.

> 직관: 소스/타깃이 “온도계 단위”가 다를 수 있으니,
> 각각을 표준화해서 “같은 단위”로 맞춘 다음 비교하는 겁니다.

---

# (4) `approach_c.py` — 핵심 알고리즘 (메타데이터 기반 적응 + 메타러닝 + 프로토타입)

## 4-1. 한 문장으로

**‘조건 변화는 이상이 아니다’를 학습시키기 위해, 메타데이터(속도/노이즈/마이크/섹션 등)를 보조 분류 과제로 만들어 메타러닝으로 학습하고, 새로운 조건에서는 정상 3개만으로 빠르게 적응한 뒤, 정상 프로토타입(중심점)에서 얼마나 멀리 떨어지는지로 이상 점수를 만든다.**

Chen et al.(2022) 계열 연구는 “3-shot으로 새 조건에 적응”하고, “메타정보(기기 모델/부하 등)를 활용한 auxiliary classification task”를 사용한다고 명시합니다.

---

## 4-2. 여기서 말하는 메타데이터는 “정확히 무엇”인가 (현재 구현 기준)

`out_c_metrics.json` config에는 task가 다음 4개로 박혀 있습니다:

* `section_id`
* `speed_level`
* `noise_id`
* `mic_position_id` 

즉, 이 구현은 “소리 자체”만 보지 않고:

* 어떤 섹션(개체)인지,
* 어떤 속도 조건인지,
* 어떤 노이즈 조건인지,
* 마이크 위치가 어디인지

를 **구분/설명 가능한 정보**로 취급합니다.

---

## 4-3. 학습(메타러닝)은 어떤 방식인가 (Reptile + episodic)

config를 보면:

* `train_mode="reptile"`
* `train_steps=800`
* `inner_steps=5`
* `n_way=4`, `k_shot=3`, `q_query=5` 

이를 쉬운 말로 풀면:

1. 매 스텝마다 “오늘은 mic_position 맞추기”, “오늘은 speed_level 맞추기”… 같은 **보조 분류 과제(task)**를 하나 뽑습니다.
2. 그 task에서 몇 개 클래스(n_way)를 뽑고,
3. 각 클래스당 k_shot만큼 support를 뽑아, **몇 번(inner_steps)만 빠르게 학습**해봅니다.
4. 그런 “빠른 학습”이 여러 task에서 잘 되도록, Reptile이 **원래 파라미터를 ‘빠르게 적응 가능한 방향’으로 조금씩 이동**시킵니다.

> 직관: “시험공부”가 아니라, **‘새 문제를 빨리 푸는 법(적응하는 법)’을 연습**시키는 겁니다.

---

## 4-4. 타깃 적응은 어떻게 하나 (few-shot 적응)

config에 적응 관련 파라미터가 존재합니다:

* `adapt_mode="prototypes"`
* `adapt_steps=10`, `adapt_lr=0.0005`
* 최종 스코어 방식: `score_mode="min_dist"`
* task 합치기: `task_agg="mean"` 

작동을 step-by-step으로 설명하면:

1. 타깃 도메인 정상 3개(=support)를 받습니다. 
2. (선택적으로) support로 몇 번(adapt_steps) 업데이트해서 타깃 조건에 맞추고,
3. 각 task마다 support 임베딩을 모아 **프로토타입(centroid)**을 만듭니다.
4. 테스트 샘플 임베딩 (z)가 들어오면, 각 task에서 “가장 가까운 프로토타입까지의 거리”를 구합니다:

[
\text{score}_\text{task}(z) = \min_c |z - \mu_c|
]

5. task가 여러 개니까 평균으로 합칩니다:

[
\text{score}(z) = \frac{1}{T}\sum_{\text{task}} \text{score}_\text{task}(z)
]

> 직관: 정상은 “정상 중심점들” 근처에 있어야 합니다.
> 그런데 조건이 바뀌면 중심점 자체가 이동하니, few-shot으로 그 중심점을 빠르게 다시 잡는 겁니다.

---

## 4-5. 직관 예시(왜 메타데이터가 도움이 되나)

* 팬이 정상인데도, **마이크가 반대편으로 옮겨지면** 소리가 크게 달라집니다.
* 메타데이터가 없다면 모델은 “이상이다!”라고 오탐낼 수 있습니다.
* C는 “마이크 위치”라는 task를 통해
  **‘마이크가 바뀌어도 정상일 수 있다’**를 학습/적응하려고 합니다.

---

# 마지막 요약: 4개 파일이 “한 세트로” 만드는 핵심 가치

* `toy_dummy_data.py`는 **few-shot + 도메인 시프트**라는 “현장 난제”를 축소판으로 만들어,
* A/B/C 세 알고리즘이 각각 어떤 철학으로 그 난제를 푸는지,
* 그리고 “어디서 깨지는지(노이즈/마이크/속도)”를 **메타데이터 기반으로 설명 가능**하게 해줍니다.

그리고 A/B/C의 핵심은 이렇게 한 줄로 기억하면 됩니다:

* **A:** “학습 없이” 정상 패치들과 **유사도**로 이상 탐지 (Frozen AST)
* **B:** 정상 임베딩 **메모리뱅크 + kNN 거리**, 타깃 부족은 **MemMixup(λ=0.9)**, 도메인 차이는 **Z-score DN**
* **C:** 메타데이터를 task로 만들어 **“적응하는 법”을 메타러닝**, 타깃에서는 프로토타입 거리로 이상 점수

